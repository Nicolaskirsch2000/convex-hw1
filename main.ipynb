{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6c6cdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from cvxpy.atoms.affine.wraps import psd_wrap\n",
    "from read_data import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%       MGT - 418         %%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%      Convex Optimization - Project 2          %%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%             2021-2022 Fall                    %%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%      Learning the Kernel Function             %%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f14d7ae",
   "metadata": {},
   "source": [
    "### SVM functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5852e647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_learning(K1, K2, K3, y_tr, rho):\n",
    "    \"\"\"\n",
    "    Kernel learning for soft margin SVM. \n",
    "    Implementation of problem (5)\n",
    "    Use cvxpy.atoms.affine.psd_wrap for each G(\\hat K^l) matrix when it appear in the constraints and in the objective\n",
    "    \"\"\"\n",
    "        \n",
    "    lambda_ = cp.Variable(len(y_tr))\n",
    "    z = cp.Variable(1)\n",
    "    \n",
    "    #TODO : Check : here trace of K train\n",
    "    c = np.trace(K1 + K2 + K3)\n",
    "    \n",
    "    cons = []\n",
    "    for k_i in [K1,K2,K3] : \n",
    "        G = np.diag(y_tr) @ k_i @ np.diag(y_tr)\n",
    "        r = np.trace(k_i)\n",
    "        cons.append(z * r >= 1/ (2 * rho) * cp.quad_form(lambda_, psd_wrap(G)))\n",
    "    cons.append(lambda_<= 1)\n",
    "    cons.append(lambda_>=0)\n",
    "    cons.append(lambda_.T @ y_tr == 0)\n",
    "    \n",
    "    obj = cp.Maximize(lambda_.T @ np.ones(len(y_tr)) - c*z)\n",
    "    \n",
    "    \n",
    "    prob = cp.Problem(obj, cons)\n",
    "    prob.solve(solver=cp.MOSEK)\n",
    "\n",
    "    \n",
    "    mu_opt1 = cons[0].dual_value\n",
    "    mu_opt2 = cons[1].dual_value\n",
    "    mu_opt3 = cons[2].dual_value\n",
    "\n",
    "    \n",
    "    b_opt = cons[5].dual_value\n",
    "    return mu_opt1, mu_opt2, mu_opt3, lambda_.value, b_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbb8392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_fit(kernel, y_tr, rho):\n",
    "    \"\"\"\n",
    "    Dual of soft-margin SVM problem (2)\n",
    "    Use cvxpy.atoms.affine.psd_wrap for each G(\\hat K^l) matrix when it appear in the constraints and in the objective\n",
    "    \"\"\"\n",
    "    n_tr = len(y_tr)\n",
    "    G =  np.diag(y_tr) @ kernel @ np.diag(y_tr)\n",
    "    lambda_ = cp.Variable(n_tr)\n",
    "    dual_obj = cp.Maximize(lambda_.T @ np.ones(n_tr) - (1/(2*rho))* cp.quad_form(lambda_, psd_wrap(G)))\n",
    "    cons = []\n",
    "    cons.append(lambda_.T @ y_tr == 0)\n",
    "    cons.append(lambda_<= 1)\n",
    "    cons.append(lambda_>=0)\n",
    "    \n",
    "    prob = cp.Problem(dual_obj, cons)\n",
    "    prob.solve(solver=cp.MOSEK)\n",
    "    lambda_opt = lambda_.value\n",
    "    b_opt =  cons[0].dual_value\n",
    "    return lambda_opt, b_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7df96cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_predict(kernel, y_tr, y_te, lambda_opt, b_opt, rho):\n",
    "    \"\"\"\n",
    "    Predict function for kernel SVM. \n",
    "    See lecture slide 183.\n",
    "    \"\"\"\n",
    "    n_te = len(y_te)\n",
    "    n_tr = len(y_tr)\n",
    "    good = 0\n",
    "    \n",
    "    for i in range(n_te): \n",
    "        tot = 0\n",
    "        for j in range(n_tr): \n",
    "            tot = tot + lambda_opt[j]*y_tr[j]*kernel[j,n_tr + i]\n",
    "        if int(np.sign((1/rho)*tot + b_opt)) == y_te[i] : \n",
    "            good = good + 1\n",
    "  \n",
    "    acc = good/n_te\n",
    "    return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c03d844",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36349fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.a\n",
    "#Split data\n",
    "def split_train_test(data, label, seed=0, ratio=0.8):\n",
    "    msk = np.random.rand(data.shape[0]) <= ratio\n",
    "\n",
    "    X_train = data[msk]\n",
    "    X_test = data[~msk]\n",
    "    y_train = label[msk]\n",
    "    y_test = label[~msk]\n",
    "    return X_train, y_train, X_test, y_test, msk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6840c56",
   "metadata": {},
   "source": [
    "### Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12b924e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.b \n",
    "\n",
    "def k_1(x,y, arg): \n",
    "    k_1 = (1.0 + x.T @ y)**arg\n",
    "    return k_1\n",
    "\n",
    "def k_2(x,y,arg) : \n",
    "    k_2 = np.exp(-np.dot((x-y).T,(x-y))/(2 * arg))\n",
    "    return k_2\n",
    "\n",
    "def k_3(x,y,arg): \n",
    "    k_3 = np.dot(x.T,y)\n",
    "    return k_3\n",
    "\n",
    "'''\n",
    "Create the Kernel matrix given the kernel function\n",
    "by iterating over the elements\n",
    "'''\n",
    "def K_creator(X, k_func,arg): \n",
    "    size = X.shape[0]\n",
    "    K = np.zeros((size , size))\n",
    "    for i in range(size) : \n",
    "        for j in range(size): \n",
    "            K[i,j] = k_func(X[i,:],X[j,:],arg)\n",
    "    return K\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a41bcdc",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "234ef756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu opt1 : 0.13262286391818937\n",
      "mu opt2 : 219.03449349418506\n",
      "mu opt3 : 1.2296413202623362\n"
     ]
    }
   ],
   "source": [
    "data, label = prepare_ionosphere_dataset()\n",
    "\n",
    "X_train, y_train, X_test, y_test, mask = split_train_test(data, label)\n",
    "\n",
    "n_tr = len(X_train)\n",
    "\n",
    "K_func = [k_1,k_2, k_3]\n",
    "rho1 = 2\n",
    "p = 2\n",
    "sigma = 0.5\n",
    "args = [p, sigma, None]\n",
    "\n",
    "x_all = np.vstack([X_train, X_test])\n",
    "\n",
    "K = [None]*3\n",
    "for i in range(3):\n",
    "    K[i] = K_creator(x_all,K_func[i],args[i])\n",
    "\n",
    "\n",
    "mu_opt1, mu_opt2, mu_opt3, lambda_opt, b_opt = kernel_learning(K[0][0: n_tr, 0: n_tr]\n",
    "                                                               ,K[1][0: n_tr, 0: n_tr]\n",
    "                                                               ,K[2][0: n_tr, 0: n_tr]\n",
    "                                                               ,y_train,rho1)\n",
    "\n",
    "print(\"mu opt1 : \" + str(mu_opt1[0]))\n",
    "print( \"mu opt2 : \" + str(mu_opt2[0]))\n",
    "print( \"mu opt3 : \" + str(mu_opt3[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "603ea879",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is : 0.9354838709677419\n"
     ]
    }
   ],
   "source": [
    "#4.c\n",
    "kernel = mu_opt1*K[0] + mu_opt2*K[1]+ mu_opt3*K[2]\n",
    "\n",
    " \n",
    "accuracy = svm_predict(kernel, y_train, y_test,lambda_opt, b_opt, rho1)\n",
    "\n",
    "print(\"The accuracy is : \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01562b47",
   "metadata": {},
   "source": [
    "### Task 5 and 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "940e9145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load and create Kernels from data...\n",
      "Begining to iterate...\n",
      "Iteration-->0\n",
      "Iteration-->1\n",
      "Iteration-->2\n",
      "Iteration-->3\n",
      "Iteration-->4\n",
      "Iteration-->5\n",
      "Iteration-->6\n",
      "Iteration-->7\n",
      "Iteration-->8\n",
      "Iteration-->9\n",
      "Iteration-->10\n",
      "Iteration-->11\n",
      "Iteration-->12\n",
      "Iteration-->13\n",
      "Iteration-->14\n",
      "Iteration-->15\n",
      "Iteration-->16\n",
      "Iteration-->17\n",
      "Iteration-->18\n",
      "Iteration-->19\n",
      "Iteration-->20\n",
      "Iteration-->21\n",
      "Iteration-->22\n",
      "Iteration-->23\n",
      "Iteration-->24\n",
      "Iteration-->25\n",
      "Iteration-->26\n",
      "Iteration-->27\n",
      "Iteration-->28\n",
      "Iteration-->29\n",
      "Iteration-->30\n",
      "Iteration-->31\n",
      "Iteration-->32\n",
      "Iteration-->33\n",
      "Iteration-->34\n",
      "Iteration-->35\n",
      "Iteration-->36\n",
      "Iteration-->37\n",
      "Iteration-->38\n",
      "Iteration-->39\n",
      "Iteration-->40\n",
      "Iteration-->41\n",
      "Iteration-->42\n",
      "Iteration-->43\n",
      "Iteration-->44\n",
      "Iteration-->45\n",
      "Iteration-->46\n",
      "Iteration-->47\n",
      "Iteration-->48\n",
      "Iteration-->49\n",
      "Iteration-->50\n",
      "Iteration-->51\n",
      "Iteration-->52\n",
      "Iteration-->53\n",
      "Iteration-->54\n",
      "Iteration-->55\n",
      "Iteration-->56\n",
      "Iteration-->57\n",
      "Iteration-->58\n",
      "Iteration-->59\n",
      "Iteration-->60\n",
      "Iteration-->61\n",
      "Iteration-->62\n",
      "Iteration-->63\n",
      "Iteration-->64\n",
      "Iteration-->65\n",
      "Iteration-->66\n",
      "Iteration-->67\n",
      "Iteration-->68\n",
      "Iteration-->69\n",
      "Iteration-->70\n",
      "Iteration-->71\n",
      "Iteration-->72\n",
      "Iteration-->73\n",
      "Iteration-->74\n",
      "Iteration-->75\n",
      "Iteration-->76\n",
      "Iteration-->77\n",
      "Iteration-->78\n",
      "Iteration-->79\n",
      "Iteration-->80\n",
      "Iteration-->81\n",
      "Iteration-->82\n",
      "Iteration-->83\n",
      "Iteration-->84\n",
      "Iteration-->85\n",
      "Iteration-->86\n",
      "Iteration-->87\n",
      "Iteration-->88\n",
      "Iteration-->89\n",
      "Iteration-->90\n",
      "Iteration-->91\n",
      "Iteration-->92\n",
      "Iteration-->93\n",
      "Iteration-->94\n",
      "Iteration-->95\n",
      "Iteration-->96\n",
      "Iteration-->97\n",
      "Iteration-->98\n",
      "Iteration-->99\n",
      "Average dual accuracy with optimal kernel is 0.9424669549490237\n",
      "Average dual accuracy with polynomial kernel is 0.9092392167021977\n",
      "Average dual accuracy with gaussian kernel is 0.8944505976203329\n",
      "Average dual accuracy with linear kernel is 0.8745068077114149\n"
     ]
    }
   ],
   "source": [
    "acc_opt_kernel = []    \n",
    "acc_poly_kernel = []    \n",
    "acc_gauss_kernel = []    \n",
    "acc_linear_kernel = []    \n",
    "\n",
    "print(\"load and create Kernels from data...\")\n",
    "data, labels = prepare_ionosphere_dataset()\n",
    "\n",
    "#Tuning hyperparameters\n",
    "rho = 2\n",
    "ratio = 0.8\n",
    "p = 2\n",
    "sigma = 0.5\n",
    "\n",
    "K1_ = K_creator(data,k_1, p)\n",
    "K2_ = K_creator(data,k_2, sigma)\n",
    "K3_ = K_creator(data,k_3,None)\n",
    "\n",
    "ind = np.arange(data.shape[0])\n",
    "\n",
    "print(\"Begining to iterate...\")\n",
    "for iters in range(100): \n",
    "    ## Please do not change the random seed.\n",
    "    np.random.seed(iters)\n",
    "    ### Training-test split\n",
    "    msk = np.random.rand(data.shape[0]) <= ratio\n",
    "    x_tr = data[msk]\n",
    "    x_te = data[~msk]\n",
    "    y_tr = labels[msk]\n",
    "    y_te = labels[~msk]\n",
    " \n",
    "    n_tr = y_tr.shape[0]\n",
    "    n_te = y_te.shape[0]\n",
    "    \n",
    "    x_all = np.vstack([x_tr, x_te])\n",
    "    n_all = x_all.shape[0]\n",
    "    \n",
    "    #Rearrange kernels rows and columns according to the new indices from x_train and x_test\n",
    "    ind_train = ind[msk]\n",
    "    ind_test = ind[~msk]\n",
    "    ind_all = np.concatenate((ind_train, ind_test), axis=0)\n",
    "    K1 = K1_[ind_all,:][:,ind_all]\n",
    "    K2 = K2_[ind_all,:][:,ind_all]\n",
    "    K3 = K3_[ind_all,:][:,ind_all]\n",
    "    \n",
    "\n",
    "    ## Prepare the initial choice of kernels \n",
    "    # It is recommended to prepare the kernels for all the training and the test data\n",
    "    # Then, the kernel size will be (n_tr + n_te)x(n_tr + n_te).\n",
    "    # Use only the training block (like K1[0:n_tr, 0:n_tr] ) to learn the classifier \n",
    "    # (for the functions svm_fit and kernel_learning).\n",
    "    # When predicting you may use the whole kernel as it is. \n",
    "    \n",
    "\n",
    "    mu_opt1, mu_opt2, mu_opt3, lambda_opt, b_opt = kernel_learning(K1[0:n_tr, 0:n_tr],\n",
    "                                                                   K2[0:n_tr, 0:n_tr],\n",
    "                                                                   K3[0:n_tr, 0:n_tr],\n",
    "                                                                   y_tr, rho)\n",
    "    #optimal kernel approach\n",
    "    opt_kernel = mu_opt1*K1 + mu_opt2*K2 + mu_opt3*K3\n",
    "    acc_opt_kernel.append(svm_predict(opt_kernel,y_tr,y_te,lambda_opt,b_opt,rho))\n",
    "    \n",
    "    #Only kernel 1\n",
    "    lambda_opt, b_opt = svm_fit(K1[0:n_tr, 0:n_tr],y_tr, rho)\n",
    "    acc_poly_kernel.append(svm_predict(K1,y_tr,y_te,lambda_opt,b_opt, rho))\n",
    "    \n",
    "    #Only kernel 2\n",
    "    lambda_opt, b_opt = svm_fit(K2[0:n_tr, 0:n_tr],y_tr, rho)\n",
    "    acc_gauss_kernel.append(svm_predict(K2,y_tr,y_te,lambda_opt,b_opt, rho))\n",
    "    \n",
    "    #Only kernel 3\n",
    "    lambda_opt, b_opt = svm_fit(K3[0:n_tr, 0:n_tr],y_tr, rho)\n",
    "    acc_linear_kernel.append(svm_predict(K3,y_tr,y_te,lambda_opt,b_opt, rho))\n",
    "    print('Iteration-->' + str(iters))\n",
    "    \n",
    "print('Average dual accuracy with optimal kernel is ' + str(np.mean(acc_opt_kernel)))\n",
    "print('Average dual accuracy with polynomial kernel is ' + str(np.mean(acc_poly_kernel)))\n",
    "print('Average dual accuracy with gaussian kernel is ' + str(np.mean(acc_gauss_kernel)))\n",
    "print('Average dual accuracy with linear kernel is ' + str(np.mean(acc_linear_kernel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4fdedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0205c34e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
