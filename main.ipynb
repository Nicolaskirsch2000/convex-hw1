{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70c01b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from cvxpy.atoms.affine.wraps import psd_wrap\n",
    "from read_data import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%       MGT - 418         %%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%      Convex Optimization - Project 2          %%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%             2021-2022 Fall                    %%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%      Learning the Kernel Function             %%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "23721993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_learning(K1, K2, K3, y_tr, rho):\n",
    "    \"\"\"\n",
    "    Kernel learning for soft margin SVM. \n",
    "    Implementation of problem (5)\n",
    "    Use cvxpy.atoms.affine.psd_wrap for each G(\\hat K^l) matrix when it appear in the constraints and in the objective\n",
    "    \"\"\"\n",
    "        \n",
    "    lambda_ = cp.Variable(len(y_tr))\n",
    "    z = cp.Variable(1)\n",
    "    \n",
    "    c = np.trace(K1+K2+K3)\n",
    "    \n",
    "    cons = []\n",
    "    K = [K1,K2,K3]\n",
    "    for k_i in K : \n",
    "        cons.append(z * np.trace(k_i) >= 1/ (2 * rho) * cp.quad_form(lambda_, psd_wrap(np.diag(y_tr) @ k_i @ np.diag(y_tr))))\n",
    "    cons.append(lambda_<= 1)\n",
    "    cons.append(lambda_>=0)\n",
    "    cons.append(lambda_.T @ y_tr == 0)\n",
    "    \n",
    "    obj = cp.Maximize(cp.vstack([lambda_.T]) @ np.ones(len(y_tr)) - c*z)\n",
    "    \n",
    "    \n",
    "    prob = cp.Problem(obj, cons)\n",
    "    prob.solve(solver=cp.MOSEK)\n",
    "\n",
    "    \n",
    "    mu_opt1 = cons[0].dual_value\n",
    "    mu_opt2 = cons[1].dual_value\n",
    "    mu_opt3 = cons[2].dual_value\n",
    "\n",
    "    \n",
    "    b_opt = cons[5].dual_value\n",
    "    return mu_opt1, mu_opt2, mu_opt3, lambda_.value, b_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "704fcbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_fit(kernel, y_tr, rho):\n",
    "    \"\"\"\n",
    "    Dual of soft-margin SVM problem (2)\n",
    "    Use cvxpy.atoms.affine.psd_wrap for each G(\\hat K^l) matrix when it appear in the constraints and in the objective\n",
    "    \"\"\"\n",
    "    n_tr = len(y_tr)\n",
    "    G =  np.diag(y_tr) @ kernel @ np.diag(y_tr)\n",
    "    lambda_ = cp.Variable(n_tr)\n",
    "    dual_obj = cp.Maximize(cp.sum(cp.vstack([lambda_])) - (1/2*rho)* cp.quad_form(lambda_, psd_wrap(G)))\n",
    "    cons = []\n",
    "    cons.append(lambda_.T @ y_tr == 0)\n",
    "    cons.append(lambda_<= 1)\n",
    "    cons.append(lambda_>=0)\n",
    "    \n",
    "    prob = cp.Problem(dual_obj, cons)\n",
    "    prob.solve(solver=cp.MOSEK)\n",
    "    lambda_opt = lambda_.value\n",
    "    b_opt =  cons[0].dual_value\n",
    "    return lambda_opt, b_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "85af84e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_predict(kernel, y_tr, y_te, lambda_opt, b_opt, rho):\n",
    "    \"\"\"\n",
    "    Predict function for kernel SVM. \n",
    "    See lecture slide 183.\n",
    "    \"\"\"\n",
    "    n_te = len(y_te)\n",
    "    n_tr = len(y_tr)\n",
    "    good = 0\n",
    "    \n",
    "    for i in range(n_te): \n",
    "        tot = 0\n",
    "        for j in range(n_tr): \n",
    "            tot = tot + lambda_opt[j]*y_tr[j]*kernel[i,j] + b_opt\n",
    "        #print(\"/\")\n",
    "        #tot = 1\n",
    "        if int(np.sign((1/rho)*tot)) == y_te[i] : \n",
    "            \n",
    "            good = good + 1\n",
    "  \n",
    "    acc = good/n_te\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7ad9d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.a\n",
    "data, label = prepare_ionosphere_dataset()\n",
    "\n",
    "msk = np.random.rand(a[0].shape[0]) <= 0.8\n",
    "X_train = data[msk]\n",
    "X_test = data[~msk]\n",
    "y_train = label[msk]\n",
    "y_test = label[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e0a708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.b \n",
    "\n",
    "def k_1(x,y, arg): \n",
    "    k_1 = (1.0 + np.dot(x.T,y))**int(arg)\n",
    "    return k_1\n",
    "\n",
    "def k_2(x,y,arg) : \n",
    "    k_2 = np.exp(-np.dot((x-y).T,(x-y))/2*0.5)\n",
    "    return k_2\n",
    "\n",
    "def k_3(x,y,arg): \n",
    "    k_3 = np.dot(x.T,y)\n",
    "    return k_3\n",
    "\n",
    "def K_creator(X_train, k_func,arg): \n",
    "    K = np.zeros((X_train.shape[0],X_train.shape[0]))\n",
    "    for i in range(X_train.shape[0]) : \n",
    "        for j in range(X_train.shape[0]): \n",
    "            K[i,j] = k_func(X_train[i,:],X_train[j,:],arg)\n",
    "    return K\n",
    "\n",
    "K_func = [k_1,k_2, k_3]\n",
    "args = [2.0,0.5,None]\n",
    "K = [None]*3\n",
    "for i in range(3):\n",
    "    K[i] = K_creator(X_train,K_func[i],args[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "96d3517a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu opt1 : 1.0411825399208081e-07\n",
      "mu opt2 : 268.1433539203483\n",
      "mu opt3 : 2.098807295265703e-06\n"
     ]
    }
   ],
   "source": [
    "mu_opt1, mu_opt2, mu_opt3, lambda_opt, b_opt = kernel_learning(K[0],K[1],K[2],y_train,2)\n",
    "\n",
    "print(\"mu opt1 : \" + str(mu_opt1[0]))\n",
    "print( \"mu opt2 : \" + str(mu_opt2[0]))\n",
    "print( \"mu opt3 : \" + str(mu_opt3[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "0a4d8f73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is : 0.42105263157894735\n"
     ]
    }
   ],
   "source": [
    "#4.c\n",
    "kernel = mu_opt1*K[0] + mu_opt2*K[1]+ mu_opt3*K[2]\n",
    "\n",
    "accuracy = svm_predict(kernel, y_train,y_test,lambda_opt,b_opt, 2)\n",
    "\n",
    "print(\"The accuracy is : \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "dfed2468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration-->0\n",
      "Iteration-->1\n",
      "Iteration-->2\n",
      "Iteration-->3\n",
      "Iteration-->4\n",
      "Iteration-->5\n",
      "Iteration-->6\n",
      "Iteration-->7\n",
      "Iteration-->8\n",
      "Iteration-->9\n",
      "Iteration-->10\n",
      "Iteration-->11\n",
      "Iteration-->12\n",
      "Iteration-->13\n",
      "Iteration-->14\n",
      "Iteration-->15\n",
      "Iteration-->16\n",
      "Iteration-->17\n",
      "Iteration-->18\n",
      "Iteration-->19\n",
      "Iteration-->20\n",
      "Iteration-->21\n",
      "Iteration-->22\n",
      "Iteration-->23\n",
      "Iteration-->24\n",
      "Iteration-->25\n",
      "Iteration-->26\n",
      "Iteration-->27\n",
      "Iteration-->28\n",
      "Iteration-->29\n",
      "Iteration-->30\n",
      "Iteration-->31\n",
      "Iteration-->32\n",
      "Iteration-->33\n",
      "Iteration-->34\n",
      "Iteration-->35\n",
      "Iteration-->36\n",
      "Iteration-->37\n",
      "Iteration-->38\n",
      "Iteration-->39\n",
      "Iteration-->40\n",
      "Iteration-->41\n",
      "Iteration-->42\n",
      "Iteration-->43\n",
      "Iteration-->44\n",
      "Iteration-->45\n",
      "Iteration-->46\n",
      "Iteration-->47\n",
      "Iteration-->48\n",
      "Iteration-->49\n",
      "Iteration-->50\n",
      "Iteration-->51\n",
      "Iteration-->52\n",
      "Iteration-->53\n",
      "Iteration-->54\n",
      "Iteration-->55\n",
      "Iteration-->56\n",
      "Iteration-->57\n",
      "Iteration-->58\n",
      "Iteration-->59\n",
      "Iteration-->60\n",
      "Iteration-->61\n",
      "Iteration-->62\n",
      "Iteration-->63\n",
      "Iteration-->64\n",
      "Iteration-->65\n",
      "Iteration-->66\n",
      "Iteration-->67\n",
      "Iteration-->68\n",
      "Iteration-->69\n",
      "Iteration-->70\n",
      "Iteration-->71\n",
      "Iteration-->72\n",
      "Iteration-->73\n",
      "Iteration-->74\n",
      "Iteration-->75\n",
      "Iteration-->76\n",
      "Iteration-->77\n",
      "Iteration-->78\n",
      "Iteration-->79\n",
      "Iteration-->80\n",
      "Iteration-->81\n",
      "Iteration-->82\n",
      "Iteration-->83\n",
      "Iteration-->84\n",
      "Iteration-->85\n",
      "Iteration-->86\n",
      "Iteration-->87\n",
      "Iteration-->88\n",
      "Iteration-->89\n",
      "Iteration-->90\n",
      "Iteration-->91\n",
      "Iteration-->92\n",
      "Iteration-->93\n",
      "Iteration-->94\n",
      "Iteration-->95\n",
      "Iteration-->96\n",
      "Iteration-->97\n",
      "Iteration-->98\n",
      "Iteration-->99\n",
      "Average dual accuracy with optimal kernel is 0.35906645822437083\n",
      "Average dual accuracy with polynomial kernel is 0.38701122116678005\n",
      "Average dual accuracy with gaussian kernel is 0.35906645822437083\n",
      "Average dual accuracy with linear kernel is 0.35906645822437083\n"
     ]
    }
   ],
   "source": [
    "#5 and 6\n",
    "acc_opt_kernel = []    \n",
    "acc_poly_kernel = []    \n",
    "acc_gauss_kernel = []    \n",
    "acc_linear_kernel = []    \n",
    "rho = 0.01\n",
    "data, labels = prepare_ionosphere_dataset()\n",
    "for iters in range(100): \n",
    "    ## Please do not change the random seed.\n",
    "    np.random.seed(iters)\n",
    "    ### Training-test split\n",
    "    msk = np.random.rand(data.shape[0]) <= 0.2\n",
    "    x_tr = data[msk]\n",
    "    x_te = data[~msk]\n",
    "    y_tr = labels[msk]\n",
    "    y_te = labels[~msk]\n",
    " \n",
    "    n_tr = y_tr.shape[0]\n",
    "    n_te = y_te.shape[0]\n",
    "    n_tr = x_tr.shape[0]\n",
    "    n_te = x_te.shape[0]\n",
    "    \n",
    "    x_all = np.vstack([x_tr, x_te])\n",
    "    n_all = x_all.shape[0]\n",
    "\n",
    "    ## Prepare the initial choice of kernels \n",
    "    # It is recommended to prepare the kernels for all the training and the test data\n",
    "    # Then, the kernel size will be (n_tr + n_te)x(n_tr + n_te).\n",
    "    # Use only the training block (like K1[0:n_tr, 0:n_tr] ) to learn the classifier \n",
    "    # (for the functions svm_fit and kernel_learning).\n",
    "    # When predicting you may use the whole kernel as it is. \n",
    "    K1 = K_creator(x_all,k_1,2)\n",
    "    K2 = K_creator(x_all,k_2,0.5)\n",
    "    K3 = K_creator(x_all,k_3,None)\n",
    "\n",
    "    mu_opt1, mu_opt2, mu_opt3, lambda_opt, b_opt = kernel_learning(K1[0:n_tr, 0:n_tr],\n",
    "                                                                   K2[0:n_tr, 0:n_tr],\n",
    "                                                                   K3[0:n_tr, 0:n_tr],\n",
    "                                                                   y_tr,2)\n",
    "    opt_kernel = mu_opt1*K1 + mu_opt2*K2 + mu_opt3*K3\n",
    "    acc_opt_kernel.append(svm_predict(opt_kernel,y_tr,y_te,lambda_opt,b_opt,2))\n",
    "    \n",
    "    lambda_opt, b_opt = svm_fit(K1[0:n_tr, 0:n_tr],y_tr,2)\n",
    "    acc_poly_kernel.append(svm_predict(K1,y_tr,y_te,lambda_opt,b_opt,2))\n",
    "    \n",
    "    lambda_opt, b_opt = svm_fit(K2[0:n_tr, 0:n_tr],y_tr,2)\n",
    "    acc_gauss_kernel.append(svm_predict(K2,y_tr,y_te,lambda_opt,b_opt,2))\n",
    "    \n",
    "    lambda_opt, b_opt = svm_fit(K3[0:n_tr, 0:n_tr],y_tr,2)\n",
    "    acc_linear_kernel.append(svm_predict(K3,y_tr,y_te,lambda_opt,b_opt,2))\n",
    "    print('Iteration-->' + str(iters))\n",
    "print('Average dual accuracy with optimal kernel is ' + str(np.mean(acc_opt_kernel)))\n",
    "print('Average dual accuracy with polynomial kernel is ' + str(np.mean(acc_poly_kernel)))\n",
    "print('Average dual accuracy with gaussian kernel is ' + str(np.mean(acc_gauss_kernel)))\n",
    "print('Average dual accuracy with linear kernel is ' + str(np.mean(acc_linear_kernel)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
