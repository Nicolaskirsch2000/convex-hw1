{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70c01b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from cvxpy.atoms.affine.wraps import psd_wrap\n",
    "from read_data import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%       MGT - 418         %%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%      Convex Optimization - Project 2          %%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%             2021-2022 Fall                    %%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%      Learning the Kernel Function             %%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23721993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_learning(K1, K2, K3, y_tr, rho):\n",
    "    \"\"\"\n",
    "    Kernel learning for soft margin SVM. \n",
    "    Implementation of problem (5)\n",
    "    Use cvxpy.atoms.affine.psd_wrap for each G(\\hat K^l) matrix when it appear in the constraints and in the objective\n",
    "    \"\"\"\n",
    "    \n",
    "    #Y = np.outer(y_tr,y_tr.T)\n",
    "    \n",
    "    lambda_ = cp.Variable(len(y_tr))\n",
    "    z = cp.Variable(1)\n",
    "    \n",
    "    c = np.trace(K1+K2+K3)\n",
    "    \n",
    "    \n",
    "   \n",
    "    cons = []\n",
    "    K = [K1,K2,K3]\n",
    "    for k_i in K : \n",
    "        cons.append(z * np.trace(k_i) >= 1/ (2 * rho) * cp.quad_form(lambda_, psd_wrap(np.diag(y_tr) * k_i * np.diag(y_tr))))\n",
    "    cons.append(lambda_<= 1)\n",
    "    cons.append(lambda_>=0)\n",
    "    cons.append(lambda_.T @ y_tr == 0)\n",
    "    \n",
    "    obj = cp.Maximize(cp.vstack([lambda_.T]) @ np.ones(len(y_tr)) -c*z)\n",
    "    \n",
    "    \n",
    "    print(\"ole\")\n",
    "    prob = cp.Problem(obj, cons)\n",
    "    prob.solve(solver=cp.MOSEK)\n",
    "\n",
    "    \n",
    "    mu_opt1 = cons[0].dual_value\n",
    "    mu_opt2 = cons[1].dual_value\n",
    "    mu_opt3 = cons[2].dual_value\n",
    "\n",
    "    \n",
    "    b_opt = cons[5].dual_value\n",
    "    return mu_opt1, mu_opt2, mu_opt3, lambda_.value, b_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704fcbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_fit(kernel, y_tr, rho):\n",
    "    \"\"\"\n",
    "    Dual of soft-margin SVM problem (2)\n",
    "    Use cvxpy.atoms.affine.psd_wrap for each G(\\hat K^l) matrix when it appear in the constraints and in the objective\n",
    "    \"\"\"\n",
    "    n_tr = len(y_tr)\n",
    "    G =  ...\n",
    "    lambda_ = cp.Variable(n_tr)\n",
    "    dual_obj = cp.Maximize(... cp.quad_form(lambda_, psd_wrap(G)))\n",
    "    cons = []\n",
    "    ...\n",
    "    prob = cp.Problem(dual_obj, cons)\n",
    "    prob.solve(solver=cp.MOSEK)\n",
    "    lambda_opt = lambda_.value\n",
    "    b_opt =  ...\n",
    "    return lambda_opt, b_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0eea9ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_predict(kernel, y_tr, y_te, lambda_opt, b_opt, rho):\n",
    "    \"\"\"\n",
    "    Predict function for kernel SVM. \n",
    "    See lecture slide 183.\n",
    "    \"\"\"\n",
    "    n_te = len(y_te)\n",
    "    n_tr = len(y_tr)\n",
    "    good = 0\n",
    "    for i in range(n_te): \n",
    "        tot = 0\n",
    "        for j in range(n_tr): \n",
    "            tot = tot + lambda_opt[j]*y_tr[j]*kernel[i,j] + b_opt\n",
    "            \n",
    "        if np.sign((1/rho)*tot) == y_te[i] : \n",
    "            good = good + 1\n",
    "  \n",
    "    acc = good/n_te\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ad9d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = prepare_ionosphere_dataset()\n",
    "X_train, X_test, y_train, y_test = train_test_split(a[0],a[1],test_size = 0.2, random_state=0)\n",
    "\n",
    "def k_1(x,y, arg): \n",
    "    k_1 = (1.0 + np.dot(x.T,y))**int(arg)\n",
    "    return k_1\n",
    "\n",
    "def k_2(x,y,arg) : \n",
    "    k_2 = np.exp(-np.dot((x-y).T,(x-y))/2*0.5)\n",
    "    return k_2\n",
    "\n",
    "def k_3(x,y,arg): \n",
    "    k_3 = np.dot(x.T,y)\n",
    "    return k_3\n",
    "\n",
    "def K_creator(X_train, k_func,arg): \n",
    "    K = np.zeros((X_train.shape[0],X_train.shape[0]))\n",
    "    for i in range(X_train.shape[0]) : \n",
    "        for j in range(X_train.shape[0]): \n",
    "            K[i,j] = k_func(X_train[i,:],X_train[j,:],arg)\n",
    "    return K\n",
    "K_func = [k_1,k_2, k_3]\n",
    "args = [2.0,0.5,None]\n",
    "K = [None]*3\n",
    "for i in range(3):\n",
    "    K[i] = K_creator(X_train,K_func[i],args[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "96d3517a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ole\n",
      "0.10935620189555241\n",
      "241.6629189479908\n",
      "3.602580529048555e-05\n",
      "280\n"
     ]
    }
   ],
   "source": [
    "result = kernel_learning(K[0],K[1],K[2],y_train,2)\n",
    "result\n",
    "\n",
    "print( result[0][0])\n",
    "print( result[1][0])\n",
    "print( result[2][0])\n",
    "print( len(result[3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0a4d8f73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5774647887323944"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = result[0][0]*K[0] + K[1]*result[1][0]+K[2]*result[2][0]\n",
    "kernel\n",
    "\n",
    "svm_predict(kernel, y_train,y_test,result[3],result[4], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dfed2468",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_opt_kernel = []    \n",
    "acc_poly_kernel = []    \n",
    "acc_gauss_kernel = []    \n",
    "acc_linear_kernel = []    \n",
    "rho = 0.01\n",
    "data, labels = prepare_ionosphere_dataset()\n",
    "for iters in range(100): \n",
    "    ## Please do not change the random seed.\n",
    "    np.random.seed(iters)\n",
    "    ### Training-test split\n",
    "    msk = np.random.rand(data_normalized.shape[0]) <=...\n",
    "    x_tr = data[...]\n",
    "    x_te = data[...]\n",
    "    y_tr = labels[...]\n",
    "    y_te = labels[...]\n",
    " \n",
    "    n_tr = y_tr.shape[0]\n",
    "    n_te = y_te.shape[0]\n",
    "    n_tr = x_tr.shape[0]\n",
    "    n_te = x_te.shape[0]\n",
    "    \n",
    "    x_all = np.vstack([x_tr, x_te])\n",
    "    n_all = x_all.shape[0]\n",
    "\n",
    "    ## Prepare the initial choice of kernels \n",
    "    # It is recommended to prepare the kernels for all the training and the test data\n",
    "    # Then, the kernel size will be (n_tr + n_te)x(n_tr + n_te).\n",
    "    # Use only the training block (like K1[0:n_tr, 0:n_tr] ) to learn the classifier \n",
    "    # (for the functions svm_fit and kernel_learning).\n",
    "    # When predicting you may use the whole kernel as it is. \n",
    "    K1 = ...\n",
    "    K2 = ...\n",
    "    K3 = ...\n",
    "\n",
    "    mu_opt1, mu_opt2, mu_opt3, lambda_opt, b_opt = kernel_learning(...)\n",
    "    opt_kernel = ...\n",
    "    acc_opt_kernel.append(svm_predict(...))\n",
    "    \n",
    "    lambda_opt, b_opt = svm_fit(...)\n",
    "    acc_poly_kernel.append(svm_predict(...))\n",
    "    \n",
    "    lambda_opt, b_opt = svm_fit(...)\n",
    "    acc_gauss_kernel.append(svm_predict(...))\n",
    "    \n",
    "    lambda_opt, b_opt = svm_fit(...)\n",
    "    acc_linear_kernel.append(svm_predict(...)\n",
    "    print('Iteration-->' + str(iters))\n",
    "print('Average dual accuracy with optimal kernel is ' + str(np.mean(acc_opt_kernel)))\n",
    "print('Average dual accuracy with polynomial kernel is ' + str(np.mean(acc_poly_kernel)))\n",
    "print('Average dual accuracy with gaussian kernel is ' + str(np.mean(acc_gauss_kernel)))\n",
    "print('Average dual accuracy with linear kernel is ' + str(np.mean(acc_linear_kernel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e3f757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
